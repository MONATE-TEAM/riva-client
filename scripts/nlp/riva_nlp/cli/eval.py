#!/usr/bin/env python
import argparse
import csv
import os.path
import pickle
import sys
import time
import warnings

from riva_nlp import BertIntentSlotClient
from sklearn.exceptions import UndefinedMetricWarning
from sklearn.metrics import classification_report
from sklearn.preprocessing import MultiLabelBinarizer
from transformers import BertTokenizer


def print_result(result, duration=0):
    print("Inference complete in {:.4f} ms".format(duration * 1000))
    print("Intent: {} ({:.4f})".format(result[0], result[1]))
    print("Slots:", result[2])
    print("Slots Scores:", result[4])
    if len(result[3]) > 0:
        print("Combined: ", end="")
        for token, slot in zip(result[3], result[2]):
            print("{}{}".format(token, "(" + slot + ")" if slot != 'O' else ""), end=" ")
        print("\n")


def display_report(truth, predicted):
    print("*" * 60)
    warnings.filterwarnings("ignore", category=UndefinedMetricWarning)
    print("classification report for Intents:")
    print(classification_report(truth["intents"], predicted["intents"]))
    print("*" * 60)

    # get classification report of multilabel
    mlb = MultiLabelBinarizer()
    all_slots = [
        {ele for row in truth["slots"] for ele in row}.union({ele for row in predicted["slots"] for ele in row})
    ]
    mlb.fit_transform(all_slots)
    y_truth = mlb.transform(truth["slots"])
    y_pred = mlb.transform(predicted["slots"])
    target_names = mlb.classes_

    print("classification report for Slots:")
    print(classification_report(y_truth, y_pred, target_names=target_names))
    print("*" * 60)


def combine_subwords(tokens):
    """This function combines subwords into single word

  Parameters
  ----------
  line : list
      input tokens generated using BERT tokenizer which may have subwords
      separated by "##".

  Returns
  -------
  list:
      a list of combined subwords token
  """
    combine_tokens = []
    total_tokens = len(tokens)
    idx = 0

    while idx < total_tokens:
        ct = tokens[idx]
        token = ""
        if ct.startswith("##"):
            # remove last token as it needs to be combine with current token
            token += combine_tokens.pop(-1)
            token += ct.strip("##")
            idx += 1
            while idx < total_tokens:
                ct = tokens[idx]
                if ct.startswith("##"):
                    token += ct.strip("##")
                else:
                    idx = idx - 1  # put back the token
                    break
                idx += 1
        else:
            token = ct
        combine_tokens.append(token)
        idx += 1

    # print("combine_tokens=", combine_tokens)
    return combine_tokens


def parse_tsv_line(line, bert_tokenizer):
    """This function parses line in tsv format.
  The expected input in tsv format is as described,
  domain:intent \t entity_start_index:entity_end_index:entity_label \t sentence

  Parameters
  ----------
  line : list
      input line data to be parse, first item contains intent,
      second item contains entities with start and end indexes
      and third item has the query with BOS/EOS marker

  Returns
  -------
  list:
      a list of intents
  string:
      query sentence
  list:
      list of tokens generated by BertTokenizer
  list:
      a list of entities
  """

    intent = line[0]
    entities_col = line[1]
    query = line[2]

    if entities_col:
        split_entities = entities_col.split(',')
        entity_list = [x.rsplit(":") for x in split_entities]
    else:
        entity_list = []

    # get tokens using BERT tokenizer
    tokenized_query = bert_tokenizer.tokenize(query)

    # combine subwords into single token
    tokens = combine_subwords(tokenized_query)

    # initialise all slots to 'O' label
    slots = ['O' for x in tokens]

    slot_index = 0
    for entity in entity_list:
        start_idx = int(entity[0])
        end_idx = int(entity[1])

        chunk = query[start_idx:end_idx]
        tokenized_chunk = bert_tokenizer.tokenize(chunk)
        combine_chunks = combine_subwords(tokenized_chunk)

        # print("tokenized_chunk=", tokenized_chunk)
        # print("combine_chunks=", combine_chunks)

        for ci, ch in enumerate(combine_chunks):
            while ch != tokens[slot_index]:
                slot_index += 1

            if ci == 0:
                tag = "B-"
            else:
                tag = "I-"

            slots[slot_index] = tag + entity[2]
            slot_index += 1

    # print("slots=",slots)

    return intent, query, tokens, slots


def handle_tsv_file(input_file, client):
    with open(input_file) as tsvfile:
        reader = csv.reader(tsvfile, delimiter='\t')
        qi = 0

        words_to_remove = ["BOS", "EOS"]
        truth = {"intents": [], "slots": []}
        predicted = {"intents": [], "slots": []}
        total_time = 0

        # get tokens using BERT tokenizer
        bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')

        for row in reader:
            # print(row)
            # print("Doman={}, Intent={}, Query={}".format(row[0], row[1],row[2]))

            intent, query, truth_tokens, slots = parse_tsv_line(row, bert_tokenizer)

            # remove BOS EOS
            truth_tokens = truth_tokens[1 : len(slots) - 1]
            slots = slots[1 : len(slots) - 1]

            query = " ".join([x for x in query.split() if x not in words_to_remove])

            truth["intents"].append(intent)
            truth["slots"].append(slots)

            # print("Count={}, Input query=[{}]".format(qi, query))
            # send query to server
            start_time = time.monotonic()
            result = client.run(query)[0]
            stop_time = time.monotonic() - start_time
            total_time += stop_time

            predicted["intents"].append(result[0])
            predicted_slots = result[2]
            predicted_tokens = result[3]

            new_pred_slot = []
            new_truth_index = 0

            # get predicted slots for prefix or exact match
            for pi in range(len(predicted_tokens)):
                for ti in range(new_truth_index, len(truth_tokens)):
                    if truth_tokens[ti].startswith(predicted_tokens[pi]):
                        new_pred_slot.append(predicted_slots[pi])
                        new_truth_index = ti
                        break
            predicted["slots"].append(new_pred_slot)

            qi += 1
            # if qi >= 10:
            # break

        display_report(truth, predicted)

        print("\nTotal Time for {0} requests is {1} ms".format(qi, total_time * 1000))
        print("Average time per request = {} ms".format(total_time / qi * 1000))


def handle_pickle_file(input_file, client):
    with open(input_file, "rb") as fh:
        data = pickle.load(fh)

        token_rev = {y: x for x, y in data[1]['token_ids'].items()}
        intent_rev = {y: x for x, y in data[1]['intent_ids'].items()}
        slot_rev = {y: x for x, y in data[1]['slot_ids'].items()}

        words_to_remove = ["BOS", "EOS"]
        truth = {"intents": [], "slots": []}
        predicted = {"intents": [], "slots": []}

        qi = 0
        total_time = 0
        for query_ids, intent_ids, slot_ids in zip(data[0]['query'], data[0]['intent_labels'], data[0]['slot_labels']):
            query = " ".join([token_rev[x] for x in query_ids if token_rev[x] not in words_to_remove])
            intents = [intent_rev[x] for x in intent_ids]
            slots = [slot_rev[x] for x in slot_ids]
            # strip first and last slot of BOS and EOS
            slots = slots[1 : len(slots) - 1]

            truth["intents"].append(intents[0])
            truth["slots"].append(slots)

            truth_tokens = []
            for t in query_ids:
                truth_tokens.append(token_rev[t])

            # remove first and last token
            truth_tokens = truth_tokens[1 : len(truth_tokens) - 1]

            # print("Count={}, Input query=[{}]".format(qi, query))
            # send query to server
            start_time = time.monotonic()
            result = client.run(query)[0]
            stop_time = time.monotonic() - start_time
            total_time += stop_time

            predicted["intents"].append(result[0])
            predicted_slots = result[2]
            predicted_tokens = result[3]

            new_pred_slot = []
            new_truth_index = 0

            # get predicted slots for prefix or exact match
            for pi in range(len(predicted_tokens)):
                for ti in range(new_truth_index, len(truth_tokens)):
                    if truth_tokens[ti].startswith(predicted_tokens[pi]):
                        new_pred_slot.append(predicted_slots[pi])
                        new_truth_index = ti
                        break
            predicted["slots"].append(new_pred_slot)

            qi += 1
            # if ( qi >= 10):
            #   break

        display_report(truth, predicted)

        print("\nTotal Time for {0} requests is {1} ms".format(qi, total_time * 1000))
        print("Average time per request = {} ms".format(total_time / qi * 1000))


def get_args():
    parser = argparse.ArgumentParser(description="Program to print accuracy metrics for test data")
    parser.add_argument("--server", default="localhost:50051", type=str, help="URI to GRPC server endpoint")
    parser.add_argument("--model", default="atis_intent_slot", type=str, help="Model on TRTIS to execute")
    parser.add_argument("--input_file", type=str, required=True, help="Input filename")
    parser.add_argument("--ssl_cert", type=str, default="", help="Path to SSL client certificatates file")
    parser.add_argument(
        "--use_ssl", default=False, action='store_true', help="Boolean to control if SSL/TLS encryption should be used"
    )
    return parser.parse_args()


def eval_main():
    args = get_args()
    client = BertIntentSlotClient(args.server, model_name=args.model, use_ssl=args.use_ssl, ssl_cert=args.ssl_cert)
    if args.input_file.split(".")[-1] == "pkl":
        handle_pickle_file(args.input_file, client)
    elif args.input_file.split(".")[-1] == "tsv":
        handle_tsv_file(args.input_file, client)
    else:
        print("Unsupported file passed")
