{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c97d9d54",
   "metadata": {},
   "source": [
    "# ASR API tutorial\n",
    "\n",
    "This tutorial demonstates how to use Python Riva API.\n",
    "\n",
    "## <font color=\"blue\">Server</font>\n",
    "\n",
    "Before running client part of Riva, please set up a server. The simplest\n",
    "way to do this is to follow\n",
    "[quick start guide](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/quick-start-guide.html#local-deployment-using-quick-start-scripts).\n",
    "\n",
    "\n",
    "## <font color=\"blue\">Authentication</font>\n",
    "\n",
    "Before using Riva services you will need to establish connection with a server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd57f561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import riva_api\n",
    "\n",
    "uri = \"localhost:50051\"  # Default value\n",
    "\n",
    "auth = riva_api.Auth(uri=uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5319eae",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Setting up service</font>\n",
    "\n",
    "To instantiate a service pass `riva_api.Auth` instance to a constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb75ba38",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_service = riva_api.ASRService(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4f6a1c",
   "metadata": {},
   "source": [
    "For speech recognition you will need to create a recognition config (an instance of `riva_api.RecognitionConfig`). \n",
    "A detailed description of config fields is available in Riva \n",
    "[documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/reference/protos/riva_asr.proto.html?highlight=max%20alternatives#riva-proto-riva-asr-proto).\n",
    "If you intend to use streaming recognition, an offline config has to wrapped into `riva_api.StreamingRecognitionConfig`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94e0e2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "offline_config = riva_api.RecognitionConfig(\n",
    "    encoding=riva_api.AudioEncoding.LINEAR_PCM,\n",
    "    max_alternatives=1,\n",
    "    enable_automatic_punctuation=True,\n",
    "    verbatim_transcripts=False,\n",
    ")\n",
    "streaming_config = riva_api.StreamingRecognitionConfig(config=deepcopy(offline_config), interim_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e975f38",
   "metadata": {},
   "source": [
    "You also need to a set frame rate and number of channels of audio which is going to be processed. If you'd like to process file `examples/en-US_AntiBERTa_for_word_boosting_testing.wav`, then your code will be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3723b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_wav_file = '../examples/en-US_AntiBERTa_for_word_boosting_testing.wav'\n",
    "riva_api.add_audio_file_specs_to_config(offline_config, my_wav_file)\n",
    "riva_api.add_audio_file_specs_to_config(streaming_config, my_wav_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8175e1d",
   "metadata": {},
   "source": [
    "If you intent to use word boosting, then use convenience method `riva_api.add_word_boosting_to_config()` to add boosting parameters to config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6aff022",
   "metadata": {},
   "outputs": [],
   "source": [
    "boosted_lm_words = ['AntiBERTa', 'ABlooper']\n",
    "boosted_lm_score = 20.0\n",
    "riva_api.add_word_boosting_to_config(offline_config, boosted_lm_words, boosted_lm_score)\n",
    "riva_api.add_word_boosting_to_config(streaming_config, boosted_lm_words, boosted_lm_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7818f6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding: LINEAR_PCM\n",
      "sample_rate_hertz: 48000\n",
      "max_alternatives: 1\n",
      "speech_contexts {\n",
      "  phrases: \"AntiBERTa\"\n",
      "  phrases: \"ABlooper\"\n",
      "  boost: 20.0\n",
      "}\n",
      "audio_channel_count: 1\n",
      "enable_automatic_punctuation: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(offline_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cdfc64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config {\n",
      "  encoding: LINEAR_PCM\n",
      "  sample_rate_hertz: 48000\n",
      "  max_alternatives: 1\n",
      "  speech_contexts {\n",
      "    phrases: \"AntiBERTa\"\n",
      "    phrases: \"ABlooper\"\n",
      "    boost: 20.0\n",
      "  }\n",
      "  audio_channel_count: 1\n",
      "  enable_automatic_punctuation: true\n",
      "}\n",
      "interim_results: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(streaming_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c87b82b",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Offline</font>\n",
    "\n",
    "To run offline speech recognition read data from a file and pass to a service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "539c95f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(my_wav_file, 'rb') as fh:\n",
    "    data = fh.read()\n",
    "\n",
    "response = asr_service.offline_recognize(data, offline_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35834e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results {\n",
      "  alternatives {\n",
      "    transcript: \"AntiBERTa and ABlooper, both transformer based language models are examples of the emerging work in using graph networks to design protein sequences for particular target antigens. \"\n",
      "    confidence: 1.0\n",
      "  }\n",
      "  channel_tag: 1\n",
      "  audio_processed: 14.762687683105469\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdd9348",
   "metadata": {},
   "source": [
    "To extract a transcript you may use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76532974",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AntiBERTa and ABlooper, both transformer based language models are examples of the emerging work in using graph networks to design protein sequences for particular target antigens. \n"
     ]
    }
   ],
   "source": [
    "print(response.results[0].alternatives[0].transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaead038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(response.results[0].alternatives[0].confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5254e8",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Streaming</font>\n",
    "\n",
    "To imitate audio streaming use `riva_api.AudioChunkFileIterator`. You can imitate realtime audio by providing a delay callback to the iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1eb7d7a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 96000\n",
      "1 96000\n",
      "2 96000\n",
      "3 96000\n",
      "4 96000\n",
      "5 96000\n",
      "6 96000\n",
      "7 96000\n",
      "8 96000\n",
      "9 96000\n",
      "10 96000\n",
      "11 96000\n",
      "12 96000\n",
      "13 96000\n",
      "14 73216\n"
     ]
    }
   ],
   "source": [
    "wav_parameters = riva_api.get_wav_file_parameters(my_wav_file)\n",
    "# correponds to 1 second of audio\n",
    "chunk_size = wav_parameters['framerate']\n",
    "with riva_api.AudioChunkFileIterator(\n",
    "    my_wav_file, chunk_size, delay_callback=riva_api.sleep_audio_length,\n",
    ") as audio_chunk_iterator:\n",
    "    for i, chunk in enumerate(audio_chunk_iterator):\n",
    "        print(i, len(chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b146f07",
   "metadata": {},
   "source": [
    "Then audio chunks are passed to `ASRService.streaming_response_generator()` and response generator is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a6cce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_chunk_iterator = riva_api.AudioChunkFileIterator(my_wav_file, 4800)\n",
    "response_generator = asr_service.streaming_response_generator(audio_chunk_iterator, streaming_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c8ee10",
   "metadata": {},
   "source": [
    "You may find description of streaming response (`StreamingRecognizeResponse`) fields in Riva [documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/reference/protos/riva_asr.proto.html?highlight=max%20alternatives#riva-proto-riva-asr-proto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f401240",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_response = next(response_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9df74c",
   "metadata": {},
   "source": [
    "For showing streaming results it is convenient to use function `riva_api.print_streaming()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ecdc5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>Time 1654013111.81s: ant\n",
      ">>>Time 1654013111.82s: anti bird\n",
      ">>>Time 1654013111.83s: anti berta\n",
      ">>>Time 1654013111.83s: auntie berta and\n",
      ">>>Time 1654013111.85s: auntie berta and\n",
      ">>>Time 1654013111.85s: auntie berta and abe\n",
      ">>>Time 1654013111.85s: AntiBERTa and abu\n",
      ">>>Time 1654013111.85s: AntiBERTa and abu po\n",
      ">>>Time 1654013111.86s: auntie bertha and ABlooper\n",
      ">>>Time 1654013111.86s: berta and ABlooper\n",
      ">>>Time 1654013111.87s: berta and aber\n",
      ">>>Time 1654013111.87s: berta and abu both\n",
      ">>>Time 1654013111.87s: anti and ABlooper both\n",
      ">>>Time 1654013112.05s: anti and ABlooper both\n",
      ">>>Time 1654013112.06s: AntiBERTa and ABlooper both strong\n",
      ">>>Time 1654013112.06s: AntiBERTa and ABlooper both transform\n",
      ">>>Time 1654013112.07s: AntiBERTa and a looper both transform\n",
      ">>>Time 1654013112.09s: AntiBERTa and both transform a basic\n",
      ">>>Time 1654013112.09s: AntiBERTa and both transform a baseline\n",
      ">>>Time 1654013112.09s: AntiBERTa and both transform a base language\n",
      ">>>Time 1654013112.10s: AntiBERTa and ABlooper both transform a base language\n",
      ">>>Time 1654013112.10s: AntiBERTa and ABlooper both transformer based language mobile\n",
      ">>>Time 1654013112.10s: AntiBERTa and ABlooper both transformer based language models\n",
      ">>>Time 1654013112.11s: AntiBERTa and ABlooper transformer based language models\n",
      ">>>Time 1654013112.11s: AntiBERTa and ABlooper transformer based language models\n",
      ">>>Time 1654013112.11s: AntiBERTa and ABlooper both based language models\n",
      ">>>Time 1654013112.12s: AntiBERTa and ABlooper both based language models are\n",
      ">>>Time 1654013112.41s: AntiBERTa and ABlooper both based language models are exact\n",
      ">>>Time 1654013112.67s: AntiBERTa and ABlooper both based language models are examples\n",
      ">>>Time 1654013112.67s: AntiBERTa and ABlooper both transformer language models are examples\n",
      ">>>Time 1654013112.68s: AntiBERTa and ABlooper both transformer language models are examples of the\n",
      ">>>Time 1654013112.68s: AntiBERTa and ABlooper both transformer based models are examples of the\n",
      ">>>Time 1654013112.68s: AntiBERTa and ABlooper both transformer based models are examples of the emerge\n",
      ">>>Time 1654013112.69s: AntiBERTa and ABlooper both transformer based language are examples of the emerging\n",
      ">>>Time 1654013112.70s: AntiBERTa and ABlooper both transformer based language are examples of the emerging world\n",
      ">>>Time 1654013112.70s: AntiBERTa and ABlooper both transformer based language are examples of the emerging work\n",
      ">>>Time 1654013112.71s: AntiBERTa and ABlooper both transformer based language are examples of the emerging work\n",
      ">>>Time 1654013112.71s: AntiBERTa and ABlooper both transformer based language models examples of the emerging work new\n",
      ">>>Time 1654013112.71s: AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using\n",
      ">>>Time 1654013112.71s: AntiBERTa and ABlooper both transformer based language models are of the emerging work in using\n",
      ">>>Time 1654013112.72s: AntiBERTa and ABlooper both transformer based language models are of the emerging work in using graph\n",
      ">>>Time 1654013112.72s: AntiBERTa and ABlooper both transformer based language models are examples the emerging work in using graph\n",
      ">>>Time 1654013112.73s: AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph to and\n",
      ">>>Time 1654013112.73s: AntiBERTa and ABlooper both transformer based language models are examples of the ging work in using graph to and network\n",
      ">>>Time 1654013112.74s: AntiBERTa and ABlooper both transformer based language models are examples of the ging work in using graph to and network\n",
      ">>>Time 1654013112.74s: AntiBERTa and ABlooper both transformer based language models are examples of the work in using graph to networks\n",
      ">>>Time 1654013112.74s: AntiBERTa and ABlooper both transformer based language models are examples of the emerging in using graph tel networks\n",
      ">>>Time 1654013112.75s: AntiBERTa and ABlooper both transformer based language models are examples of the emerging in using graph tel networks\n",
      ">>>Time 1654013112.75s: AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph to networks to\n",
      ">>>Time 1654013112.76s: AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in graph tel networks to design\n",
      ">>>Time 1654013112.76s: AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in graph t networks to design\n",
      ">>>Time 1654013112.77s: AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using tel networks to design total\n",
      ">>>Time 1654013112.77s: AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using tal networks to design proteins\n",
      ">>>Time 1654013112.77s: AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using networks to design protein\n",
      ">>>Time 1654013112.78s: AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using networks to design protein sequence\n",
      ">>>Time 1654013112.78s: AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using networks to design protein sequences\n",
      ">>>Time 1654013112.78s: AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph to design protein sequences\n",
      ">>>Time 1654013112.79s: AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph to design protein sequences\n",
      ">>>Time 1654013112.79s: AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph to design protein sequences for\n",
      ">>>Time 1654013112.79s: AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks design protein sequences for\n",
      ">>>Time 1654013112.79s: AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to protein sequences for particular\n",
      ">>>Time 1654013112.80s: AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to protein sequences for particular\n",
      ">>>Time 1654013112.80s: AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to protein sequences for particular\n",
      ">>>Time 1654013112.80s: AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design tin sequences for particular target\n",
      ">>>Time 1654013112.81s: AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design sequences for particular targeted\n",
      ">>>Time 1654013112.81s: AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design sequences for particular target and\n",
      ">>>Time 1654013112.81s: AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design protein for particular target antigen\n",
      ">>>Time 1654013112.82s: AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design protein for particular target antigen\n",
      ">>>Time 1654013112.82s: AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design protein for particular target antigens\n",
      ">>>Time 1654013112.82s: AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design protein sequences for particular target antigens\n",
      ">>>Time 1654013112.82s: AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design protein sequences particular target antigens\n",
      ">>>Time 1654013112.89s: AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design protein sequences for target antigens\n",
      ">>>Time 1654013112.89s: AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design protein sequences for target antigens\n",
      ">>>Time 1654013112.89s: AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design protein sequences for target antigens\n",
      "Time 1.87s: Transcript 0: AntiBERTa and ABlooper, both transformer based language models are examples of the emerging work in using graph networks to design protein sequences for particular target antigens. \n"
     ]
    }
   ],
   "source": [
    "riva_api.print_streaming(response_generator, additional_info='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdc7afb",
   "metadata": {},
   "source": [
    "If you set a delay callback in audio chunk iterator and `show_intermediate=True` in `riva_api.print_streaming()`, then you will be able watch transcript forming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "380b2ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## AntiBERTa and ABlooper, both transformer based language models are examples of the emerging work in using graph networks to design protein sequences for particular target antigens. \n"
     ]
    }
   ],
   "source": [
    "audio_chunk_iterator = riva_api.AudioChunkFileIterator(my_wav_file, 4800, riva_api.sleep_audio_length)\n",
    "response_generator = asr_service.streaming_response_generator(audio_chunk_iterator, streaming_config)\n",
    "riva_api.print_streaming(response_generator, show_intermediate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aaea8f",
   "metadata": {},
   "source": [
    "It is also possible to print streaming results in several places, e.g. in STDOUT and a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f531607b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> ant\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> anti bird\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> anti berta\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> auntie berta and\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> auntie berta and\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> auntie berta and abe\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and abu\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and abu po\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> auntie bertha and ABlooper\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> berta and aber\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> berta and aber\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> berta and abu both\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> anti \n",
      "Stability:    0.9000\n",
      ">> and ABlooper both\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> anti \n",
      "Stability:    0.9000\n",
      ">> and ABlooper both\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa \n",
      "Stability:    0.9000\n",
      ">> and ABlooper both strong\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and \n",
      "Stability:    0.9000\n",
      ">> ABlooper both transform\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and a \n",
      "Stability:    0.9000\n",
      ">> looper both transform\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and \n",
      "Stability:    0.9000\n",
      ">> both transform a basic\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and \n",
      "Stability:    0.9000\n",
      ">> both transform a baseline\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and \n",
      "Stability:    0.9000\n",
      ">> both transform a base language\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper \n",
      "Stability:    0.9000\n",
      ">> both transform a base language\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper \n",
      "Stability:    0.9000\n",
      ">> both transformer based language mobile\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper \n",
      "Stability:    0.9000\n",
      ">> both transformer based language models\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper \n",
      "Stability:    0.9000\n",
      ">> transformer based language models\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper \n",
      "Stability:    0.9000\n",
      ">> transformer based language models\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both \n",
      "Stability:    0.9000\n",
      ">> based language models\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both \n",
      "Stability:    0.9000\n",
      ">> based language models are\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both \n",
      "Stability:    0.9000\n",
      ">> based language models are exact\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both \n",
      "Stability:    0.9000\n",
      ">> based language models are examples\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer \n",
      "Stability:    0.9000\n",
      ">> language models are examples\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer \n",
      "Stability:    0.9000\n",
      ">> language models are examples of the\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based \n",
      "Stability:    0.9000\n",
      ">> models are examples of the\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based \n",
      "Stability:    0.9000\n",
      ">> models are examples of the emerge\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language \n",
      "Stability:    0.9000\n",
      ">> are examples of the emerging\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language \n",
      "Stability:    0.9000\n",
      ">> are examples of the emerging world\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language \n",
      "Stability:    0.9000\n",
      ">> are examples of the emerging work\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language \n",
      "Stability:    0.9000\n",
      ">> are examples of the emerging work\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models \n",
      "Stability:    0.9000\n",
      ">> examples of the emerging work new\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are \n",
      "Stability:    0.9000\n",
      ">> examples of the emerging work in using\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are \n",
      "Stability:    0.9000\n",
      ">> of the emerging work in using\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are \n",
      "Stability:    0.9000\n",
      ">> of the emerging work in using graph\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples \n",
      "Stability:    0.9000\n",
      ">> the emerging work in using graph\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the \n",
      "Stability:    0.9000\n",
      ">> emerging work in using graph to and\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the \n",
      "Stability:    0.9000\n",
      ">> ging work in using graph to and network\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the \n",
      "Stability:    0.9000\n",
      ">> ging work in using graph to and network\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the \n",
      "Stability:    0.9000\n",
      ">> work in using graph to networks\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging \n",
      "Stability:    0.9000\n",
      ">> in using graph tel networks\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging \n",
      "Stability:    0.9000\n",
      ">> in using graph tel networks\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in \n",
      "Stability:    0.9000\n",
      ">> using graph to networks to\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in \n",
      "Stability:    0.9000\n",
      ">> graph tel networks to design\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in \n",
      "Stability:    0.9000\n",
      ">> graph t networks to design\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using \n",
      "Stability:    0.9000\n",
      ">> tel networks to design total\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using \n",
      "Stability:    0.9000\n",
      ">> tal networks to design proteins\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using \n",
      "Stability:    0.9000\n",
      ">> networks to design protein\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using \n",
      "Stability:    0.9000\n",
      ">> networks to design protein sequence\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using \n",
      "Stability:    0.9000\n",
      ">> networks to design protein sequences\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph \n",
      "Stability:    0.9000\n",
      ">> to design protein sequences\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph \n",
      "Stability:    0.9000\n",
      ">> to design protein sequences\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph \n",
      "Stability:    0.9000\n",
      ">> to design protein sequences for\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks \n",
      "Stability:    0.9000\n",
      ">> design protein sequences for\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to \n",
      "Stability:    0.9000\n",
      ">> protein sequences for particular\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to \n",
      "Stability:    0.9000\n",
      ">> protein sequences for particular\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to \n",
      "Stability:    0.9000\n",
      ">> protein sequences for particular\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design \n",
      "Stability:    0.9000\n",
      ">> tin sequences for particular target\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design \n",
      "Stability:    0.9000\n",
      ">> sequences for particular targeted\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design \n",
      "Stability:    0.9000\n",
      ">> sequences for particular target and\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design protein \n",
      "Stability:    0.9000\n",
      ">> for particular target antigen\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design protein \n",
      "Stability:    0.9000\n",
      ">> for particular target antigen\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design protein \n",
      "Stability:    0.9000\n",
      ">> for particular target antigens\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design protein sequences \n",
      "Stability:    0.9000\n",
      ">> for particular target antigens\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design protein sequences \n",
      "Stability:    0.9000\n",
      ">> particular target antigens\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design protein sequences for \n",
      "Stability:    0.9000\n",
      ">> target antigens\n",
      "Stability:    0.1000\n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design protein sequences for \n",
      "Stability:    0.9000\n",
      ">> target antigens\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design protein sequences for \n",
      "Stability:    0.9000\n",
      ">> target antigens\n",
      "Stability:    0.1000\n",
      "----\n",
      "## AntiBERTa and ABlooper, both transformer based language models are examples of the emerging work in using graph networks to design protein sequences for particular target antigens. \n",
      "Confidence:    1.0000\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "output_file = \"my_results.txt\"\n",
    "audio_chunk_iterator = riva_api.AudioChunkFileIterator(my_wav_file, 4800)\n",
    "response_generator = asr_service.streaming_response_generator(audio_chunk_iterator, streaming_config)\n",
    "riva_api.print_streaming(response_generator, additional_info='confidence', output_file=[sys.stdout, output_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3339c08c",
   "metadata": {},
   "source": [
    "Showing file and clean up in bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f205c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'cat' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!cat $output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a948a2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!rm $output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9addd43e",
   "metadata": {},
   "source": [
    "Showing file and clean up in cmd.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70ea3158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> ant\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> anti bird\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> anti berta\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> auntie berta and\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> auntie berta and\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> auntie berta and abe\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and abu\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and abu po\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> auntie bertha and ABlooper\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> berta and aber\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> berta and aber\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> berta and abu both\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> anti \n",
      "Stability:    0.9000\n",
      ">> and ABlooper both\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> anti \n",
      "Stability:    0.9000\n",
      ">> and ABlooper both\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa \n",
      "Stability:    0.9000\n",
      ">> and ABlooper both strong\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and \n",
      "Stability:    0.9000\n",
      ">> ABlooper both transform\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and a \n",
      "Stability:    0.9000\n",
      ">> looper both transform\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and \n",
      "Stability:    0.9000\n",
      ">> both transform a basic\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and \n",
      "Stability:    0.9000\n",
      ">> both transform a baseline\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and \n",
      "Stability:    0.9000\n",
      ">> both transform a base language\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper \n",
      "Stability:    0.9000\n",
      ">> both transform a base language\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper \n",
      "Stability:    0.9000\n",
      ">> both transformer based language mobile\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper \n",
      "Stability:    0.9000\n",
      ">> both transformer based language models\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper \n",
      "Stability:    0.9000\n",
      ">> transformer based language models\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper \n",
      "Stability:    0.9000\n",
      ">> transformer based language models\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both \n",
      "Stability:    0.9000\n",
      ">> based language models\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both \n",
      "Stability:    0.9000\n",
      ">> based language models are\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both \n",
      "Stability:    0.9000\n",
      ">> based language models are exact\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both \n",
      "Stability:    0.9000\n",
      ">> based language models are examples\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer \n",
      "Stability:    0.9000\n",
      ">> language models are examples\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer \n",
      "Stability:    0.9000\n",
      ">> language models are examples of the\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based \n",
      "Stability:    0.9000\n",
      ">> models are examples of the\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based \n",
      "Stability:    0.9000\n",
      ">> models are examples of the emerge\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language \n",
      "Stability:    0.9000\n",
      ">> are examples of the emerging\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language \n",
      "Stability:    0.9000\n",
      ">> are examples of the emerging world\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language \n",
      "Stability:    0.9000\n",
      ">> are examples of the emerging work\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language \n",
      "Stability:    0.9000\n",
      ">> are examples of the emerging work\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models \n",
      "Stability:    0.9000\n",
      ">> examples of the emerging work new\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are \n",
      "Stability:    0.9000\n",
      ">> examples of the emerging work in using\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are \n",
      "Stability:    0.9000\n",
      ">> of the emerging work in using\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are \n",
      "Stability:    0.9000\n",
      ">> of the emerging work in using graph\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples \n",
      "Stability:    0.9000\n",
      ">> the emerging work in using graph\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the \n",
      "Stability:    0.9000\n",
      ">> emerging work in using graph to and\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the \n",
      "Stability:    0.9000\n",
      ">> ging work in using graph to and network\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the \n",
      "Stability:    0.9000\n",
      ">> ging work in using graph to and network\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the \n",
      "Stability:    0.9000\n",
      ">> work in using graph to networks\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging \n",
      "Stability:    0.9000\n",
      ">> in using graph tel networks\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging \n",
      "Stability:    0.9000\n",
      ">> in using graph tel networks\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in \n",
      "Stability:    0.9000\n",
      ">> using graph to networks to\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in \n",
      "Stability:    0.9000\n",
      ">> graph tel networks to design\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in \n",
      "Stability:    0.9000\n",
      ">> graph t networks to design\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using \n",
      "Stability:    0.9000\n",
      ">> tel networks to design total\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using \n",
      "Stability:    0.9000\n",
      ">> tal networks to design proteins\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using \n",
      "Stability:    0.9000\n",
      ">> networks to design protein\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using \n",
      "Stability:    0.9000\n",
      ">> networks to design protein sequence\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using \n",
      "Stability:    0.9000\n",
      ">> networks to design protein sequences\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph \n",
      "Stability:    0.9000\n",
      ">> to design protein sequences\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph \n",
      "Stability:    0.9000\n",
      ">> to design protein sequences\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph \n",
      "Stability:    0.9000\n",
      ">> to design protein sequences for\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks \n",
      "Stability:    0.9000\n",
      ">> design protein sequences for\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to \n",
      "Stability:    0.9000\n",
      ">> protein sequences for particular\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to \n",
      "Stability:    0.9000\n",
      ">> protein sequences for particular\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to \n",
      "Stability:    0.9000\n",
      ">> protein sequences for particular\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design \n",
      "Stability:    0.9000\n",
      ">> tin sequences for particular target\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design \n",
      "Stability:    0.9000\n",
      ">> sequences for particular targeted\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design \n",
      "Stability:    0.9000\n",
      ">> sequences for particular target and\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design protein \n",
      "Stability:    0.9000\n",
      ">> for particular target antigen\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design protein \n",
      "Stability:    0.9000\n",
      ">> for particular target antigen\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design protein \n",
      "Stability:    0.9000\n",
      ">> for particular target antigens\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design protein sequences \n",
      "Stability:    0.9000\n",
      ">> for particular target antigens\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design protein sequences \n",
      "Stability:    0.9000\n",
      ">> particular target antigens\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design protein sequences for \n",
      "Stability:    0.9000\n",
      ">> target antigens\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design protein sequences for \n",
      "Stability:    0.9000\n",
      ">> target antigens\n",
      "Stability:    0.1000\n",
      "----\n",
      ">> AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design protein sequences for \n",
      "Stability:    0.9000\n",
      ">> target antigens\n",
      "Stability:    0.1000\n",
      "----\n",
      "## AntiBERTa and ABlooper, both transformer based language models are examples of the emerging work in using graph networks to design protein sequences for particular target antigens. \n",
      "Confidence:    1.0000\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "!type $output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02a8efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!del $output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b40e15",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Audio input/output</font>\n",
    "\n",
    "For using audio input and output you need to install PyAudio.\n",
    "\n",
    "```bash\n",
    "conda install -c anaconda pyaudio\n",
    "```\n",
    "\n",
    "### <font color=\"green\">Playing audio during transcribing</font>\n",
    "\n",
    "For playing audio simultaneously with transcribing, provide an instance of `riva_api.audio_io.SoundCallBack` as a `delay_callback` to `riva_api.AudioChunkFileIterator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9097d307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import riva_api.audio_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "530af21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output audio devices:\n",
      "2: Microsoft Sound Mapper - Output\n",
      "3: Speakers (Synaptics Audio)\n",
      "4: Output 1 (Synaptics Audio headphone)\n",
      "5: Output 2 (Synaptics Audio headphone)\n",
      "13: Output 1 (Synaptics Audio output)\n",
      "14: Output 2 (Synaptics Audio output)\n"
     ]
    }
   ],
   "source": [
    "# show available output devices\n",
    "riva_api.audio_io.list_output_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70f52ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## AntiBERTa and ABlooper, both transformer based language models are examples of the emerging work in using graph networks to design protein sequences for particular target antigens. \n"
     ]
    }
   ],
   "source": [
    "output_device = None  # use default device\n",
    "wav_parameters = riva_api.get_wav_file_parameters(my_wav_file)\n",
    "sound_callback = riva_api.audio_io.SoundCallBack(\n",
    "    output_device, wav_parameters['sampwidth'], wav_parameters['nchannels'], wav_parameters['framerate'],\n",
    ")\n",
    "audio_chunk_iterator = riva_api.AudioChunkFileIterator(my_wav_file, 4800, sound_callback)\n",
    "response_generator = asr_service.streaming_response_generator(audio_chunk_iterator, streaming_config)\n",
    "riva_api.print_streaming(response_generator, show_intermediate=True)\n",
    "sound_callback.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee8c861",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Streaming from microphone</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "251ed66e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input audio devices:\n",
      "0: Microsoft Sound Mapper - Input\n",
      "1: Microphone Array (Synaptics Aud\n",
      "6: Input (Synaptics Audio headphone)\n",
      "7: Microphone 1 (Synaptics Audio capture)\n",
      "8: Microphone 2 (Synaptics Audio capture)\n",
      "9: Microphone 3 (Synaptics Audio capture)\n",
      "10: Microphone Array 1 (Synaptics Audio capture)\n",
      "11: Microphone Array 2 (Synaptics Audio capture)\n",
      "12: Microphone Array 3 (Synaptics Audio capture)\n",
      "15: Input (Synaptics Audio output)\n"
     ]
    }
   ],
   "source": [
    "riva_api.audio_io.list_input_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c5a1ad",
   "metadata": {},
   "source": [
    "Run code below and then say something in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb174fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Tell me something.  \n",
      "## No. \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m input_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# default device\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m riva_api\u001b[38;5;241m.\u001b[39maudio_io\u001b[38;5;241m.\u001b[39mMicrophoneStream(\n\u001b[0;32m      3\u001b[0m     rate\u001b[38;5;241m=\u001b[39mstreaming_config\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39msample_rate_hertz,\n\u001b[0;32m      4\u001b[0m     chunk\u001b[38;5;241m=\u001b[39mstreaming_config\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39msample_rate_hertz \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m      5\u001b[0m     device\u001b[38;5;241m=\u001b[39minput_device,\n\u001b[0;32m      6\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m audio_chunk_iterator:\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mriva_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_streaming\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masr_service\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstreaming_response_generator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43maudio_chunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_chunk_iterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstreaming_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreaming_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_intermediate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\riva-python-clients\\lib\\site-packages\\riva_api\\asr.py:168\u001b[0m, in \u001b[0;36mprint_streaming\u001b[1;34m(responses, output_file, additional_info, word_time_offsets, show_intermediate, file_mode)\u001b[0m\n\u001b[0;32m    166\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# used in 'time` additional_info\u001b[39;00m\n\u001b[0;32m    167\u001b[0m num_chars_printed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# used in 'no' additional_info\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m responses:\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response\u001b[38;5;241m.\u001b[39mresults:\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\riva-python-clients\\lib\\site-packages\\riva_api\\asr.py:307\u001b[0m, in \u001b[0;36mASRService.streaming_response_generator\u001b[1;34m(self, audio_chunks, streaming_config)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;124;03mGenerates speech recognition responses for fragments of speech audio in :param:`audio_chunks`.\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;124;03mThe purpose of the method is to perform speech recognition \"online\" - as soon as\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;124;03m    message `here <https://docs.nvidia.com/deeplearning/riva/user-guide/docs/reference/protos/protos.html#riva-proto-riva-asr-proto>`_.\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    306\u001b[0m generator \u001b[38;5;241m=\u001b[39m streaming_request_generator(audio_chunks, streaming_config)\n\u001b[1;32m--> 307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstub\u001b[38;5;241m.\u001b[39mStreamingRecognize(generator, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth\u001b[38;5;241m.\u001b[39mget_auth_metadata()):\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\riva-python-clients\\lib\\site-packages\\grpc\\_channel.py:426\u001b[0m, in \u001b[0;36m_Rendezvous.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 426\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\riva-python-clients\\lib\\site-packages\\grpc\\_channel.py:817\u001b[0m, in \u001b[0;36m_MultiThreadedRendezvous._next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_response_ready\u001b[39m():\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    813\u001b[0m             (cygrpc\u001b[38;5;241m.\u001b[39mOperationType\u001b[38;5;241m.\u001b[39mreceive_message\n\u001b[0;32m    814\u001b[0m              \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mdue \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    815\u001b[0m              \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m--> 817\u001b[0m \u001b[43m_common\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcondition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_response_ready\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    819\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mresponse\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\riva-python-clients\\lib\\site-packages\\grpc\\_common.py:141\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(wait_fn, wait_complete_fn, timeout, spin_cb)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wait_complete_fn():\n\u001b[1;32m--> 141\u001b[0m         \u001b[43m_wait_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAXIMUM_WAIT_TIMEOUT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspin_cb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m timeout\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\riva-python-clients\\lib\\site-packages\\grpc\\_common.py:106\u001b[0m, in \u001b[0;36m_wait_once\u001b[1;34m(wait_fn, timeout, spin_cb)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wait_once\u001b[39m(wait_fn, timeout, spin_cb):\n\u001b[1;32m--> 106\u001b[0m     \u001b[43mwait_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spin_cb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    108\u001b[0m         spin_cb()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\riva-python-clients\\lib\\threading.py:306\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 306\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    308\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_device = None  # default device\n",
    "with riva_api.audio_io.MicrophoneStream(\n",
    "    rate=streaming_config.config.sample_rate_hertz,\n",
    "    chunk=streaming_config.config.sample_rate_hertz // 10,\n",
    "    device=input_device,\n",
    ") as audio_chunk_iterator:\n",
    "    riva_api.print_streaming(\n",
    "        responses=asr_service.streaming_response_generator(\n",
    "            audio_chunks=audio_chunk_iterator,\n",
    "            streaming_config=streaming_config,\n",
    "        ),\n",
    "        show_intermediate=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a65243d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f428d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
